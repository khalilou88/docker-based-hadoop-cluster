services:
  spark-master:
    image: apache/spark:latest
    container_name: spark-master
    restart: always
    ports:
      - "8080:8080"
      - "7077:7077"
      - "4040:4040"
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    networks:
      - hadoop-network
    command: >
      bash -c "/opt/spark/bin/spark-class org.apache.spark.deploy.master.Master 
      --host spark-master 
      --port 7077 
      --webui-port 8080"

  spark-worker-1:
    image: apache/spark:latest
    container_name: spark-worker-1
    restart: always
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_DRIVER_MEMORY=1g
      - SPARK_EXECUTOR_MEMORY=1g
      - SPARK_WORKER_WEBUI_PORT=8081
    networks:
      - hadoop-network
    command: >
      bash -c "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker 
      spark://spark-master:7077 
      --webui-port 8081
      --cores 2
      --memory 2g"

  spark-worker-2:
    image: apache/spark:latest
    container_name: spark-worker-2
    restart: always
    depends_on:
      - spark-master
    ports:
      - "8082:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_DRIVER_MEMORY=1g
      - SPARK_EXECUTOR_MEMORY=1g
      - SPARK_WORKER_WEBUI_PORT=8081
    networks:
      - hadoop-network
    command: >
      bash -c "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker 
      spark://spark-master:7077 
      --webui-port 8081
      --cores 2
      --memory 2g"

networks:
  hadoop-network:
    external: true